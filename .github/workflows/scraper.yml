name: Daily YouTube Scraper

on:
  schedule:
    - cron: '0 15 * * *'  # daily at 3PM UTC
  workflow_dispatch:

permissions:
  contents: write   # allow push

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          ref: main            # ensure we're on main
          fetch-depth: 0       # full history so pushes work
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          python -m playwright install chromium

      - name: Decode creds.json from secret
        run: echo "${{ secrets.GOOGLE_CREDS_JSON_B64 }}" | base64 --decode > creds.json

      - name: Run scraper
        run: python scrape.py

      # --- debug so we can see what's going on ---
      - name: Show working dir status
        run: |
          ls -la
          echo "----- git status -----"
          git status
          echo "----- keyword_index.txt (if present) -----"
          [ -f keyword_index.txt ] && cat keyword_index.txt || echo "keyword_index.txt not found"

      # --- commit only if file exists and changed ---
      - name: Commit updated keyword index
        run: |
          if [ ! -f keyword_index.txt ]; then
            echo "No keyword_index.txt found; skipping commit."
            exit 0
          fi

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add -- keyword_index.txt

          if git diff --cached --quiet; then
            echo "No index changes to commit."
            exit 0
          fi

          git commit -m "chore: advance keyword index [skip ci]"
          # push back to main explicitly
          git push origin HEAD:main
